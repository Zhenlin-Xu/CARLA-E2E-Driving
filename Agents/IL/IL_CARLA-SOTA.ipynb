{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8aed63-cc1a-46e1-981c-5b2fff67c362",
   "metadata": {},
   "source": [
    "## Imitation learning for CARLA Autonomous Vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c71ffa-5a67-4617-b3f6-fa04ecc0225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import yaml\n",
    "import random\n",
    "import datetime\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75a3ef62-19a6-4730-8ce8-720533107a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeds\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e736e62-4aff-4e92-9f69-b414bf99940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard init\n",
    "date_str = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
    "writer = SummaryWriter(log_dir=\"./log/\"+date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5694428-2fbc-451b-bb08-3257c46fa4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timer init\n",
    "begin = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce9f362-3ac1-42bf-9d08-32e89979956d",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7085dc3f-eaa2-48d7-aaad-cb03fbc32ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# network architecture\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(),)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(),)\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(),)\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(),)\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(),)\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(),)\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(),)\n",
    "        self.conv8 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(),)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Sequential(\n",
    "            nn.Linear(in_features=8192, out_features=512),\n",
    "            nn.ReLU())\n",
    "        self.dense2 = nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=512),\n",
    "            nn.ReLU())\n",
    "        self.output = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x) \n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57099f91-113a-433e-b88a-963cbb66899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "model = Network().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f4cac7-feeb-445a-845b-d9cebabde6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and criterion init\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80347091-52a4-4d82-9597-59b6440befd8",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a04f157b-df49-4ce1-9382-826c7ec58316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train h5: 3288\n",
      "# test  h5: 374\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(os.getcwd(), 'CORL2017ImitationLearningData/AgentHuman/SeqTrain')\n",
    "test_path  = os.path.join(os.getcwd(), 'CORL2017ImitationLearningData/AgentHuman/SeqVal')\n",
    "train_dataset_ls = os.listdir(path=train_path) # 3288\n",
    "num_train_dataset = len(train_dataset_ls)\n",
    "test_dataset_ls = os.listdir(path=test_path)   # 374\n",
    "num_test_dataset = len(test_dataset_ls)\n",
    "\n",
    "print('# train h5:', num_train_dataset)\n",
    "print('# test  h5:', num_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e3f92c-8bd8-4b47-827a-bd3db61499ef",
   "metadata": {},
   "source": [
    "### Parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18c1a86f-b7dd-4312-a05b-553b8a4318c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 200\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7134d12-cff9-49cb-a0da-9969cbdab37f",
   "metadata": {},
   "source": [
    "### Training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a7c1a2a-bf72-4397-9799-df15bd831a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss/train: 0.16378255382873042 Loss/test: 0.1697624755131808\n",
      "Epoch:  1 Loss/train: 0.1400976739197838 Loss/test: 0.15276352897595197\n",
      "Epoch:  2 Loss/train: 0.12996797470769328 Loss/test: 0.15065172588935788\n",
      "Epoch:  3 Loss/train: 0.1240373366757227 Loss/test: 0.1494168984382488\n",
      "Epoch:  4 Loss/train: 0.11913408777521302 Loss/test: 0.1473292045066143\n",
      "Epoch:  5 Loss/train: 0.11488240469540832 Loss/test: 0.1364918893419784\n",
      "Epoch:  6 Loss/train: 0.11076546749523546 Loss/test: 0.13302829428476326\n",
      "Epoch:  7 Loss/train: 0.10710594050976824 Loss/test: 0.13014971467580416\n",
      "Epoch:  8 Loss/train: 0.10415182203013923 Loss/test: 0.1226813797659616\n",
      "Epoch:  9 Loss/train: 0.10136085946741401 Loss/test: 0.11726346389674719\n",
      "Epoch: 10 Loss/train: 0.09858118187897576 Loss/test: 0.1130536557052267\n",
      "Epoch: 11 Loss/train: 0.0960742118970917 Loss/test: 0.11142298240817829\n",
      "Epoch: 12 Loss/train: 0.09370006654803005 Loss/test: 0.10804774802623884\n",
      "Epoch: 13 Loss/train: 0.09147953220423302 Loss/test: 0.10538429242254044\n",
      "Epoch: 14 Loss/train: 0.08951320998570904 Loss/test: 0.10219702927892489\n",
      "Epoch: 15 Loss/train: 0.08782050200415675 Loss/test: 0.1034367023951643\n",
      "Epoch: 16 Loss/train: 0.08581493781952339 Loss/test: 0.10375993845381719\n",
      "Epoch: 17 Loss/train: 0.0838880720972497 Loss/test: 0.0975481218432794\n",
      "Epoch: 18 Loss/train: 0.08213096491821391 Loss/test: 0.09582968954592047\n",
      "Epoch: 19 Loss/train: 0.0808088213793504 Loss/test: 0.09515893745378039\n",
      "Epoch: 20 Loss/train: 0.07910625214855313 Loss/test: 0.09194675005955577\n",
      "Epoch: 21 Loss/train: 0.07772252861047255 Loss/test: 0.0939389259774592\n",
      "Epoch: 22 Loss/train: 0.07604895303938754 Loss/test: 0.08932887153923487\n",
      "Epoch: 23 Loss/train: 0.07460382192224545 Loss/test: 0.0873739157307969\n",
      "Epoch: 24 Loss/train: 0.07336198705088641 Loss/test: 0.08491712815456968\n",
      "Epoch: 25 Loss/train: 0.07192585727069901 Loss/test: 0.0856044809628131\n",
      "Epoch: 26 Loss/train: 0.07095968518324011 Loss/test: 0.08437150273425809\n",
      "Epoch: 27 Loss/train: 0.06986115747607266 Loss/test: 0.08142064854306266\n",
      "Epoch: 28 Loss/train: 0.06868285877940838 Loss/test: 0.08130544038804778\n",
      "Epoch: 29 Loss/train: 0.06753050278513022 Loss/test: 0.08018765029952522\n",
      "Epoch: 30 Loss/train: 0.0666019038992506 Loss/test: 0.07928915942719027\n",
      "Epoch: 31 Loss/train: 0.06587099736700018 Loss/test: 0.08028467394541364\n",
      "Epoch: 32 Loss/train: 0.06508650267750213 Loss/test: 0.07760323975565171\n",
      "Epoch: 33 Loss/train: 0.0642152064355488 Loss/test: 0.07614025123041362\n",
      "Epoch: 34 Loss/train: 0.06319694095489117 Loss/test: 0.07511625354387425\n",
      "Epoch: 35 Loss/train: 0.06255552770636555 Loss/test: 0.07567049007115989\n",
      "Epoch: 36 Loss/train: 0.06180653552956665 Loss/test: 0.0740358571361322\n",
      "Epoch: 37 Loss/train: 0.06115777058546577 Loss/test: 0.07512342118866522\n",
      "Epoch: 38 Loss/train: 0.06034531574571585 Loss/test: 0.07074686837215248\n",
      "Epoch: 39 Loss/train: 0.05982458411907897 Loss/test: 0.07215080940441501\n",
      "Epoch: 40 Loss/train: 0.05939726696163515 Loss/test: 0.0698670716140339\n",
      "Epoch: 41 Loss/train: 0.05873608295671563 Loss/test: 0.06954534238025067\n",
      "Epoch: 42 Loss/train: 0.05780175909521836 Loss/test: 0.0676788608373154\n",
      "Epoch: 43 Loss/train: 0.057446185481067204 Loss/test: 0.06919931554510059\n",
      "Epoch: 44 Loss/train: 0.056818866921534816 Loss/test: 0.06568430954702342\n",
      "Epoch: 45 Loss/train: 0.05645847445160513 Loss/test: 0.0667404851085649\n",
      "Epoch: 46 Loss/train: 0.05576028975095445 Loss/test: 0.06613678312555384\n",
      "Epoch: 47 Loss/train: 0.05535836925052011 Loss/test: 0.06410999370725713\n",
      "Epoch: 48 Loss/train: 0.05455077970702998 Loss/test: 0.06530101421098648\n",
      "Epoch: 49 Loss/train: 0.05424376609789982 Loss/test: 0.06372953188807311\n",
      "Epoch: 50 Loss/train: 0.05378841899732889 Loss/test: 0.06365776409592384\n",
      "Epoch: 51 Loss/train: 0.05334652925729025 Loss/test: 0.06503775874241492\n",
      "Epoch: 52 Loss/train: 0.05263177622356255 Loss/test: 0.06142558634342168\n",
      "Epoch: 53 Loss/train: 0.052203559669260026 Loss/test: 0.06127098847289083\n",
      "Epoch: 54 Loss/train: 0.052082218604814406 Loss/test: 0.05975689306944848\n",
      "Epoch: 55 Loss/train: 0.05145913691503355 Loss/test: 0.05861635116017816\n",
      "Epoch: 56 Loss/train: 0.05089091841514313 Loss/test: 0.05940646300824935\n",
      "Epoch: 57 Loss/train: 0.05054031130152918 Loss/test: 0.058130679942568965\n",
      "Epoch: 58 Loss/train: 0.0501529351920138 Loss/test: 0.05743077230854007\n",
      "Epoch: 59 Loss/train: 0.04991496794201027 Loss/test: 0.057698840207656775\n",
      "Epoch: 60 Loss/train: 0.0492553364180806 Loss/test: 0.05729446844272515\n",
      "Epoch: 61 Loss/train: 0.04894590517865597 Loss/test: 0.057091092001885654\n",
      "Epoch: 62 Loss/train: 0.048588645183839055 Loss/test: 0.05604160509730893\n",
      "Epoch: 63 Loss/train: 0.048205699429787745 Loss/test: 0.05532082074180344\n",
      "Epoch: 64 Loss/train: 0.04778645450874126 Loss/test: 0.05525040709074925\n",
      "Epoch: 65 Loss/train: 0.047378608637357385 Loss/test: 0.05530649117295046\n",
      "Epoch: 66 Loss/train: 0.047056351268031364 Loss/test: 0.05447388629977346\n",
      "Epoch: 67 Loss/train: 0.04671743622002882 Loss/test: 0.05518321609799221\n",
      "Epoch: 68 Loss/train: 0.0461849723801927 Loss/test: 0.054520575674782995\n",
      "Epoch: 69 Loss/train: 0.04595256478136957 Loss/test: 0.05368071540114979\n",
      "Epoch: 70 Loss/train: 0.04566719354379684 Loss/test: 0.05298684166945822\n",
      "Epoch: 71 Loss/train: 0.045323428752599815 Loss/test: 0.054320077863114886\n",
      "Epoch: 72 Loss/train: 0.04503118438059819 Loss/test: 0.0526561055489011\n",
      "Epoch: 73 Loss/train: 0.044698565425121385 Loss/test: 0.05025994422132917\n",
      "Epoch: 74 Loss/train: 0.04430132381193291 Loss/test: 0.05175956536192113\n",
      "Epoch: 75 Loss/train: 0.0439852495855654 Loss/test: 0.05267884650291862\n",
      "Epoch: 76 Loss/train: 0.04379876237009005 Loss/test: 0.05261924047324057\n",
      "Epoch: 77 Loss/train: 0.0434343626800162 Loss/test: 0.05046247776396891\n",
      "Epoch: 78 Loss/train: 0.04316186277479361 Loss/test: 0.05022392715956209\n",
      "Epoch: 79 Loss/train: 0.042849905356589744 Loss/test: 0.04976418874397943\n",
      "Epoch: 80 Loss/train: 0.0424606469394773 Loss/test: 0.04890382551027208\n",
      "Epoch: 81 Loss/train: 0.0422271549732702 Loss/test: 0.048818779010733435\n",
      "Epoch: 82 Loss/train: 0.0418674296784678 Loss/test: 0.048422050447421365\n",
      "Epoch: 83 Loss/train: 0.04163175210014565 Loss/test: 0.048238071353335986\n",
      "Epoch: 84 Loss/train: 0.0413435887847149 Loss/test: 0.04913113402457345\n",
      "Epoch: 85 Loss/train: 0.04110964780052875 Loss/test: 0.048491671384548934\n",
      "Epoch: 86 Loss/train: 0.04078259400918869 Loss/test: 0.04895882880538025\n",
      "Epoch: 87 Loss/train: 0.04053670447935812 Loss/test: 0.04634273588062618\n",
      "Epoch: 88 Loss/train: 0.04030656627758345 Loss/test: 0.0474937520914324\n",
      "Epoch: 89 Loss/train: 0.039941838080300385 Loss/test: 0.04581717496478238\n",
      "Epoch: 90 Loss/train: 0.03993859124277899 Loss/test: 0.04533352464150182\n",
      "Epoch: 91 Loss/train: 0.03968688019674081 Loss/test: 0.04681386016680452\n",
      "Epoch: 92 Loss/train: 0.03928603162686156 Loss/test: 0.045614629277832545\n",
      "Epoch: 93 Loss/train: 0.0390705705573816 Loss/test: 0.04485279428253584\n",
      "Epoch: 94 Loss/train: 0.03892376194624048 Loss/test: 0.045266826332121576\n",
      "Epoch: 95 Loss/train: 0.03859151981501835 Loss/test: 0.04499761698035122\n",
      "Epoch: 96 Loss/train: 0.038260517829262305 Loss/test: 0.04477888731040648\n",
      "Epoch: 97 Loss/train: 0.03796659595943937 Loss/test: 0.042824267572458746\n",
      "Epoch: 98 Loss/train: 0.037714717781786894 Loss/test: 0.044437824298436554\n",
      "Epoch: 99 Loss/train: 0.0376486405672093 Loss/test: 0.04455558698094667\n",
      "Epoch: 100 Loss/train: 0.0372629593030003 Loss/test: 0.043419998333754185\n",
      "Epoch: 101 Loss/train: 0.037202793145109526 Loss/test: 0.042727986554674184\n",
      "Epoch: 102 Loss/train: 0.036819101662242865 Loss/test: 0.04401120816050132\n",
      "Epoch: 103 Loss/train: 0.03660505767499722 Loss/test: 0.04145325670639155\n",
      "Epoch: 104 Loss/train: 0.03636586299760942 Loss/test: 0.04195197405308019\n",
      "Epoch: 105 Loss/train: 0.03637574427700363 Loss/test: 0.04252483724187848\n",
      "Epoch: 106 Loss/train: 0.03599909312384258 Loss/test: 0.04047492393364857\n",
      "Epoch: 107 Loss/train: 0.03569540208267771 Loss/test: 0.04233257662509036\n",
      "Epoch: 108 Loss/train: 0.03571801330649788 Loss/test: 0.0411873898603692\n",
      "Epoch: 109 Loss/train: 0.035382587561712274 Loss/test: 0.04188251078326394\n",
      "Epoch: 110 Loss/train: 0.035297531969659325 Loss/test: 0.040778555188769335\n",
      "Epoch: 111 Loss/train: 0.03478341773172798 Loss/test: 0.039949071601558944\n",
      "Epoch: 112 Loss/train: 0.034966232291197746 Loss/test: 0.04145897452360653\n",
      "Epoch: 113 Loss/train: 0.03457987586857809 Loss/test: 0.03909882486038835\n",
      "Epoch: 114 Loss/train: 0.03441195022840833 Loss/test: 0.03961301213864176\n",
      "Epoch: 115 Loss/train: 0.034227627212413736 Loss/test: 0.03927958675447439\n",
      "Epoch: 116 Loss/train: 0.034048614492105445 Loss/test: 0.039318225509086334\n",
      "Epoch: 117 Loss/train: 0.03393032299704397 Loss/test: 0.03904936864269002\n",
      "Epoch: 118 Loss/train: 0.03367681819651083 Loss/test: 0.03949330919153349\n",
      "Epoch: 119 Loss/train: 0.0335778989058022 Loss/test: 0.03906504833205624\n",
      "Epoch: 120 Loss/train: 0.033515717926960654 Loss/test: 0.03825348549921509\n",
      "Epoch: 121 Loss/train: 0.033279564856359634 Loss/test: 0.038612426724712885\n",
      "Epoch: 122 Loss/train: 0.032934145372396494 Loss/test: 0.0372347248565075\n",
      "Epoch: 123 Loss/train: 0.03270725408171492 Loss/test: 0.036792664143184786\n",
      "Epoch: 124 Loss/train: 0.032599934342534075 Loss/test: 0.038218108675063894\n",
      "Epoch: 125 Loss/train: 0.032426762554297515 Loss/test: 0.03710613851240549\n",
      "Epoch: 126 Loss/train: 0.032298980662804476 Loss/test: 0.036666336233501906\n",
      "Epoch: 127 Loss/train: 0.03203340335339751 Loss/test: 0.037516379769164526\n",
      "Epoch: 128 Loss/train: 0.03198474305813684 Loss/test: 0.035925170675822064\n",
      "Epoch: 129 Loss/train: 0.031715296956135626 Loss/test: 0.037439095841514954\n",
      "Epoch: 130 Loss/train: 0.03159290321865403 Loss/test: 0.0362737163965373\n",
      "Epoch: 131 Loss/train: 0.03125390958979587 Loss/test: 0.03573417603325302\n",
      "Epoch: 132 Loss/train: 0.031137102094468707 Loss/test: 0.0365198347881792\n",
      "Epoch: 133 Loss/train: 0.031152726498723957 Loss/test: 0.03530565708881337\n",
      "Epoch: 134 Loss/train: 0.031005237679825774 Loss/test: 0.036250792742027645\n",
      "Epoch: 135 Loss/train: 0.030766582519140738 Loss/test: 0.03496186059306093\n",
      "Epoch: 136 Loss/train: 0.030510084088440848 Loss/test: 0.034699684986074265\n",
      "Epoch: 137 Loss/train: 0.03042237457645775 Loss/test: 0.035457154733598006\n",
      "Epoch: 138 Loss/train: 0.030333337833392756 Loss/test: 0.034858583607994154\n",
      "Epoch: 139 Loss/train: 0.03001219746264239 Loss/test: 0.035827372459393436\n",
      "Epoch: 140 Loss/train: 0.029976606480886403 Loss/test: 0.03395003408147987\n",
      "Epoch: 141 Loss/train: 0.029861214395296994 Loss/test: 0.03661749336338119\n",
      "Epoch: 142 Loss/train: 0.029699731061057316 Loss/test: 0.0331668280631028\n",
      "Epoch: 143 Loss/train: 0.029437695989669593 Loss/test: 0.035434527522446975\n",
      "Epoch: 144 Loss/train: 0.02932349458524518 Loss/test: 0.03437522184058398\n",
      "Epoch: 145 Loss/train: 0.02915382574338961 Loss/test: 0.033135335645834506\n",
      "Epoch: 146 Loss/train: 0.029063626360135975 Loss/test: 0.03338717364171958\n",
      "Epoch: 147 Loss/train: 0.028831859720618264 Loss/test: 0.0340625325113256\n",
      "Epoch: 148 Loss/train: 0.028653319172873434 Loss/test: 0.03438810113465951\n",
      "Epoch: 149 Loss/train: 0.028633608312363994 Loss/test: 0.03354471522385104\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    # print(\"Epoch:%4d\"  % epoch)\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    best_loss = 1\n",
    "    \n",
    "    # training \n",
    "    for file_idx in range(num_train_dataset): #num_file):\n",
    "        # a file as a batch\n",
    "        filename = os.path.join(train_path, train_dataset_ls[file_idx])\n",
    "        f = h5py.File(filename, 'r')\n",
    "        # print(list(f.keys()))\n",
    "        img = torch.Tensor(np.array(f['rgb'])).permute(0,3,2,1).cuda()\n",
    "        tar = torch.Tensor(np.array(f['targets'])).cuda()\n",
    "        \n",
    "        steer, throttle, brake = tar[:,0].unsqueeze(1), tar[:,1].unsqueeze(1), tar[:,2].unsqueeze(1)\n",
    "        ctl = torch.hstack((steer, throttle-brake))\n",
    "\n",
    "        f.close()\n",
    "        \n",
    "        model.train(True)\n",
    "        optimizer.zero_grad()       # zero the parameter gradients\n",
    "        out = model(img)\n",
    "        loss = criterion(out, ctl)  # compute the loss \n",
    "        loss.backward()             # backpropagate the loss\n",
    "        optimizer.step()            # adjust parameters based on the calculated gradients\n",
    "        train_loss += loss.item() \n",
    "        # if file_idx % 100 == 0:\n",
    "            # print(f'Epoch: {epoch:2d} [{file_idx:4d} {num_train_dataset:4d}] loss={loss.item():.5f}')\n",
    "        \n",
    "    writer.add_scalar('MSE_loss/train', train_loss/num_train_dataset, epoch)\n",
    "    if best_loss > train_loss/num_train_dataset:\n",
    "        torch.save(model.state_dict(), \"./model/model_weights_\"+date_str+'.pth')\n",
    "        torch.save(model, \"./model/model_\"+date_str+'.pth')\n",
    "        \n",
    "    # testing\n",
    "    for file_idx in range(num_test_dataset): #num_file):\n",
    "        # a file as a batch\n",
    "        filename = os.path.join(train_path, train_dataset_ls[file_idx])\n",
    "        # print(filename)\n",
    "        f = h5py.File(filename, 'r')\n",
    "        # print(list(f.keys()))\n",
    "        img = torch.Tensor(np.array(f['rgb'])).permute(0,3,2,1).cuda()\n",
    "        tar = torch.Tensor(np.array(f['targets'])).cuda()\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "        steer, throttle, brake = tar[:,0].unsqueeze(1), tar[:,1].unsqueeze(1), tar[:,2].unsqueeze(1)\n",
    "        ctl = torch.hstack((steer, throttle-brake))\n",
    "        model.train(False)\n",
    "        model.eval()\n",
    "        out = model(img)\n",
    "        loss = criterion(out, ctl)\n",
    "        test_loss += loss.item()\n",
    "        # if file_idx % 200 == 0:\n",
    "            # print(f'Epoch: {epoch:2d} [{file_idx:4d} {num_test_dataset:4d}] loss={loss.item():.5f}')\n",
    "        \n",
    "    writer.add_scalar('MSE_loss/test', test_loss/num_test_dataset, epoch)\n",
    "    writer.add_scalars('MSE_loss', {'train_loss':  train_loss/num_train_dataset, \n",
    "                                    'test_loss' :  test_loss /num_test_dataset   }\n",
    "                      ,epoch)\n",
    "    \n",
    "    print(f\"Epoch: {epoch:2d} Loss/train: {train_loss/num_train_dataset} Loss/test: {test_loss/num_test_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e11e76e4-c38f-474c-8ce3-40eab1308879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye Sir! 23510.922435760498\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Goodbye Sir!\", end - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582b3ce-6369-4175-aa1d-c3ab6901d7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
